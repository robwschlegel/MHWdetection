---
title: "Best practices"
author: "Robert Schlegel"
date: "`r Sys.Date()`"
description: "This vignette goes over the outcomes of the other vignettes to itemise best practices."
output: html_document
bibliography: bibliography.bib
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.align = 'center',
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      eval = TRUE, tidy = FALSE)
options(scipen=999)
```

## Overview

In this vignette are individual sections that show some of the closer looks performed on the results to ensure that they were behaving as expected.

```{r r-init-quiet, echo=FALSE}
.libPaths(c("~/R-packages", .libPaths()))
library(tidyverse, lib.loc = "~/R-packages/")
library(heatwaveR, lib.loc = "~/R-packages/")
library(lubridate)
library(ncdf4)
library(doParallel)
source("code/functions.R")
```

```{r r-init, eval=FALSE}
# The packages used in this analysis
library(tidyverse)
library(heatwaveR)
library(lubridate)
library(ncdf4)
library(doParallel)

# The custom functions written for the analysis
source("code/functions.R")
```


## Why are the buil-in time series so anomalous?

```{r WA-pixels, eval=FALSE}
# We'll use the WA time series and the pixel just adjacent to it
sst_WA_flat <- detrend(sst_WA) %>%
  mutate(site = "WA") %>%
  select(site, t, temp)

# Find the correct longitude slice of data and load a several pixel transect into the eye of the MHW
# which(c(seq(0.125, 179.875, by = 0.25), seq(-179.875, -0.125, by = 0.25)) == 112.375)
sst_flat <- load_noice_OISST(OISST_files[450]) %>%
  filter(lat > -29.5, lat < -27.5) %>%
  unite(lon, lat, col = "site", sep = " / ") %>%
  group_by(site) %>%
  group_modify(~detrend(.x)) %>%
  data.frame()

# Bing it to the reference time series
sst_ALL <- rbind(sst_flat, sst_WA_flat)
# unique(sst_ALL$site)
```

```{r WA-pixels-load, echo=FALSE}
# saveRDS(sst_ALL, "WA_pixels.Rda")
sst_ALL <- readRDS("analysis/WA_pixels.Rda")
```

Plot all time series together:

```{r WA-pixels-plot}
sst_ALL %>%
  filter(t >= "2010-01-01", t <= "2011-12-31") %>%
  ggplot(aes(x = t, y = temp)) +
  geom_line(aes(group = site, colour = site)) +
  scale_colour_viridis_d() +
  labs(y = "Temp. anomaly (°C)", x = NULL, colour = "Coords")
```

Run full analyses on the pixels visualised above:

```{r WA-pixels-analysis, eval=FALSE}
result_ALL <- plyr::ddply(sst_ALL, c("site"), single_analysis, full_seq = T, .parallel = T)
```

```{r WA-pixels-analysis-load, echo=FALSE}
# saveRDS(result_ALL, "WA_pixels_res.Rda")
result_ALL <- readRDS("analysis/WA_pixels_res.Rda")
```

Plot the results:

```{r res-dur-plot}
result_ALL %>%
  filter(test == "trend", var == "duration", id == "sum_perc") %>%
  ggplot(aes(x = index_vals, y = val)) +
  geom_line(aes(group = site, colour = site)) +
  scale_colour_viridis_d() +
  labs(x = "Long-term trend (°C/decade)", y = "Change from control (% sum of days)", colour = "Coords")
```

In the figure above we may see that the closer we appraoch the centre of the WA MHW the less of an effect the increasing decadal trend is having on the overall number of MHWs detected. We may deduce that this is because the WA MHW was so intense that it is raising up the 90th percentile so high that even with added decadal warming it is not enough to increase the other MHWs. Now we want to look at how the count of overall events are affected:

```{r res-count-plot}
result_ALL %>%
  filter(test == "trend", var == "count", id == "n_perc") %>%
  ggplot(aes(x = index_vals, y = val)) +
  geom_line(aes(group = site, colour = site)) +
  scale_colour_viridis_d() +
  labs(x = "Long-term trend (°C/decade)", y = "Change from control (% count of MHWs)", colour = "Coords")
```

And there you have it, the reference time series are just super wacky, the results are otherwise as expected.


## Why do some MHWs dissapear from wider windows? 

```{r window-pixels, eval=FALSE}
# -112.125 -28.875 # A pixel negatively affected by window widening
# which(c(seq(0.125, 179.875, by = 0.25), seq(-179.875, -0.125, by = 0.25)) == -112.125)
sst <- load_noice_OISST(OISST_files[992]) %>%
  filter(lat == -28.875)

# Detrend the selected ts
sst_flat <- detrend(sst)

# Calculate MHWs from detrended ts
sst_flat_MHW <- detect_event(ts2clm(sst_flat, climatologyPeriod = c("1982-01-01", "2018-12-31")))

# Pull out the largest event in the ts
focus_event <- sst_flat_MHW$event %>%
  filter(date_start >= "2009-01-01") %>%
  filter(intensity_cumulative == max(intensity_cumulative)) %>%
  select(event_no, date_start:date_end, duration, intensity_cumulative, intensity_max) %>%
  mutate(intensity_cumulative = round(intensity_cumulative, 2),
         intensity_max = round(intensity_max, 2))

# Quickly visualise the largest heatwave in the last 10 years of data
heatwaveR::event_line(sst_flat_MHW, start_date = "2009-01-01", metric = "intensity_cumulative")

# Normal window width
window_5_MHW <- detect_event(ts2clm(sst_flat, climatologyPeriod = c("1982-01-01", "2018-12-31")))
heatwaveR::event_line(window_5_MHW, start_date = "2009-01-01", metric = "intensity_cumulative")

# 10 day window
  # Already here we see why the event falls away
  # The focus MHW was just staying above the down slope of the seasonal dive into winter
  # When the window half width is expanded the seasonal decline becomes less steep and the
  # observed temperature is no longer above the 90th percentile
window_10_MHW <- detect_event(ts2clm(sst_flat, climatologyPeriod = c("1982-01-01", "2018-12-31"), windowHalfWidth = 10))
heatwaveR::event_line(window_10_MHW, start_date = "2009-01-01", metric = "intensity_cumulative")

# 20 day window
window_20_MHW <- detect_event(ts2clm(sst_flat, climatologyPeriod = c("1982-01-01", "2018-12-31"), windowHalfWidth = 20))
heatwaveR::event_line(window_20_MHW, start_date = "2009-01-01", metric = "intensity_cumulative")

# 30 day window
window_30_MHW <- detect_event(ts2clm(sst_flat, climatologyPeriod = c("1982-01-01", "2018-12-31"), windowHalfWidth = 30))
heatwaveR::event_line(window_30_MHW, start_date = "2009-01-01", metric = "intensity_cumulative")

# Now let's have a peak at each step along the way, just for laughs
ts2clm_window <- function(window_choice, df = sst_flat){
  res <- ts2clm(df, climatologyPeriod = c("1982-01-01", "2018-12-31"), windowHalfWidth = window_choice) %>%
    mutate(site_label = paste0("window_",window_choice))
  return(res)
}

# Calculate clims
sst_clim <- plyr::ldply(seq(5, 30, by = 5), ts2clm_window, .parallel = T)

# Climatologies doy
sst_clim_only <- sst_clim %>%
  select(-t, -temp) %>%
  unique()

# Calculate events
sst_event <- sst_clim %>%
  group_by(site_label) %>%
  group_modify(~detect_event(.x)$event)

# Find largest event in most recent ten years of data
focus_event <- sst_event %>%
  filter(date_start >= "2009-01-01") %>%
  group_by(site_label) %>%
  filter(intensity_cumulative == max(intensity_cumulative)) %>%
  ungroup()

# Merge with results for better plotting
sst_focus <- left_join(sst_clim,
                       focus_event[,c("site_label", "date_start", "date_peak", "date_end")], by = "site_label") %>%
  mutate(site_label = factor(site_label, levels = c("window_5", "window_10", "window_15",
                                                    "window_20", "window_25", "window_30")))

trend_fig <- fig_1_plot(sst_focus, spread = 150)
trend_fig

# Look at differences between the seas/thresh for each window
sst_clim_only %>%
  select(-doy) %>%
  gather(key = "var", value = "val", seas, thresh) %>%
  group_by(site_label, var) %>%
  summarise_if(.predicate = is.numeric, .funs = c("min", "median", "mean", "max")) %>%
  ungroup() %>%
  gather(key = "stat", value = "val", -site_label, - var) %>%
  mutate(site_label = factor(site_label, levels = c("window_5", "window_10", "window_15",
                                                    "window_20", "window_25", "window_30"))) %>%
  arrange(site_label) %>%
  ggplot(aes(x = stat, y = val, colour = site_label)) +
  geom_point() +
  scale_colour_brewer() +
  facet_wrap(~var)

# Now let's look at all of the 1000 random results to see how this shakes out
random_results <- readRDS("../data/random_results_1000.Rda")
unique(random_results$test)
all_clims <- random_results %>%
  filter(test %in% c("length", "window_10", "window_20", "window_30"),
         index_vals == 30,
         var %in% c("seas", "thresh"),
         id %in% c("min", "median", "mean", "max", "sd")) %>%
  ggplot(aes(x = id, y = val, fill = test)) +
  geom_boxplot() +
  scale_fill_brewer(palette = "YlOrRd") +
  facet_wrap(~var)
all_clims
```


## Linearity of fits

```{r, eval=FALSE}
# Load the random 1000 data
system.time(
  random_results <- readRDS("data/random_results_1000.Rda") %>%
    unite("site", c(lon, lat))
) # 68 seconds, 15 seconds without the "site" column

# The choice variables for focussing on
var_choice <- data.frame(var = c("count", "duration", "intensity_max", "focus_count", "focus_duration", "focus_intensity_max"),
                         id = c("n_perc", "sum_perc", "mean_perc", "mean_perc", "sum_perc", "mean_perc"),
                         stringsAsFactors = F)

# Calculate the full range of quantiles
random_quant <- random_results %>%
  right_join(var_choice, by = c("var", "id")) %>%
  mutate(test = as.character(test)) %>%
  filter(test %in% c("length", "missing", "interp", "trend")) %>%
  group_by(test, index_vals, var, id) %>%
  summarise(q05 = quantile(val, 0.05),
            q25 = quantile(val, 0.25),
            q50 = quantile(val, 0.50),
            q75 = quantile(val, 0.75),
            q95 = quantile(val, 0.95),
            iqr50 = q75-q25,
            iqr90 = q95-q05) %>%
  ungroup()

# Run the linear models at each possible step to deduce where any inflections points may be
# This is determined by tracking the change in R2 values, with lower values being bad
quant_missing <- plyr::ldply(3:50, trend_test, .parallel = T, test_sub = "missing", start_val = 0)
quant_missing_A <- plyr::ldply(3:25, trend_test, .parallel = T, test_sub = "missing", start_val = 0)
quant_missing_B <- plyr::ldply(3:24, trend_test, .parallel = T, test_sub = "missing", start_val = 0.26)
quant_interp <- plyr::ldply(3:50, trend_test, .parallel = T, test_sub = "interp", start_val = 0)
quant_trend <- plyr::ldply(3:30, trend_test, .parallel = T, test_sub = "trend", start_val = 0)
quant_length_A <- plyr::ldply(3:20, trend_test, .parallel = T, test_sub = "length")
quant_length_B <- plyr::ldply(3:7, trend_test, .parallel = T, test_sub = "length", rev_trend = T)
quant_ALL <- rbind(quant_missing_A, quant_missing_B, quant_interp, quant_trend, quant_length_A, quant_length_B)

## Test visuals to determine that the trends above are lekker
# First create a line plot of the results
quant_ALL %>%
  filter(test == "missing") %>%
  ggplot(aes(x = end_val, y = r2)) +
  geom_point(aes(colour = var)) +
  geom_line(aes(colour = var)) +
  facet_grid(stat~test, scales = "free_x")

# Filter out the trends that cover the correct ranges
trend_filter <- data.frame(test = c("length", "length", "missing", "missing", "interp", "trend"),
                           start_val = c(30, 30, 0, 0.26, 0, 0),
                           end_val = c(10, 37, 0.25, 0.5, 0.5, 0.3))
quant_filter <- quant_ALL %>%
  right_join(trend_filter, by = c("test", "start_val", "end_val")) %>%
  mutate(slope = ifelse(test == "length" & end_val == 10, -slope, slope),
         end_point = end_val*slope) %>%
  filter(stat != "iqr50", stat != "iqr90")

# Then project them onto the real data
ggplot(random_quant) +
  # 90 CI crossbars
  # Need different lines for tests due to the different x-axis interval sizes
  geom_crossbar(data = filter(random_quant, test == "length"),
                aes(x = index_vals, y = 0, ymin = q05, ymax = q95),
                fatten = 0, fill = "grey70", colour = NA, width = 1) +
  geom_crossbar(data = filter(random_quant, test != "length"),
                aes(x = index_vals, y = 0, ymin = q05, ymax = q95),
                fatten = 0, fill = "grey70", colour = NA, width = 0.01) +
  # IQR Crossbars
  geom_crossbar(data = filter(random_quant, test == "length"),
                aes(x = index_vals, y = 0, ymin = q25, ymax = q75),
                fatten = 0, fill = "grey50", width = 1) +
  geom_crossbar(data = filter(random_quant, test != "length"),
                aes(x = index_vals, y = 0, ymin = q25, ymax = q75),
                fatten = 0, fill = "grey50", width = 0.01) +
  # Median segments
  geom_crossbar(data = filter(random_quant, test == "length"),
                aes(x = index_vals, y = 0, ymin = q50, ymax = q50),
                fatten = 0, fill = NA, colour = "black", width = 1) +
  geom_crossbar(data = filter(random_quant, test != "length"),
                aes(x = index_vals, y = 0, ymin = q50, ymax = q50),
                fatten = 0, fill = NA, colour = "black", width = 0.01) +
  geom_hline(aes(yintercept = 0), colour = "black", linetype = "dashed") +
  geom_segment(data = quant_filter, aes(x = start_val, y = intercept, xend = end_val, yend = end_point, colour = stat)) +
  scale_colour_brewer(palette = "Dark2") +
  scale_x_continuous(expand = c(0, 0)) +
  facet_grid(var ~ test, scales = "free", switch = "both") +
  theme(legend.position = "top")
```


## References
