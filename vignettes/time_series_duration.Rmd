---
title: "Assessing the effects of time series duration"
author: "Robert W Schlegel"
date: "`r Sys.Date()`"
description: "This vignette investigates the effects of time series duration on the accurate detection of MHWs."
# output: word_document
output: rmarkdown::html_vignette
vignette: >
 %\VignetteIndexEntry{Vignette Title}
 %\VignetteEngine{knitr::rmarkdown}
 %\VignetteEncoding{UTF-8}
bibliography: bibliography.bib
---

```{r global_options, include = FALSE}
knitr::opts_chunk$set(fig.width = 8, fig.align = 'center',
                      echo = TRUE, warning = FALSE, message = FALSE, 
                      eval = TRUE, tidy = FALSE)
```


## Overview

It has been shown that the length (duration) of a time series may be one of the most important factors in determining the usefulness of those data for any number of applications [@Schlegel2016]. For the detection of MHWs it is recommended that one has at least 30 years of data in order to accurately detect the events therein. This is no longer an issue for most ocean surfaces as the high quality SST products, such as NOAA OISST, are now in exceedence of the 30-year minimum. Besides issues relating to the coarseness of these older products, there are many other reasons why one would want to detect MHWs in products with shorter records. _In situ_ measurements along the coast are one good example of this, another being the use of the newer higher resolution SST products, a third being the use of reanalysis/model products for the detection of events in 3D. It is therefore necessary to quantify the effects that shorter time series duration (< 30 years) has on the accurate detection of events. Once any discrepancies have been accounted for, best practices must be developed to allow user to improve the precision of the detection of events in their shorter time series.


## Time series shortening

A time series derives it's usefulness from it's length. This is because the greater the number of observations (larger sample size), the more confident one can be about the resultant statistics. This is particularly true for decadal scale measurements and is why the WMO recommends a minimum of 30 years of data before drawing any conclusions about decadal (long-term) trends observed in a time series. We however want to look not at decadal scale trends, but rather at how observed MHWs compare against one another when they have been detected using longer or shorter time series. In order to to quantify this effect we will use the minimum and maximum range of time series available to us. The maximum length will be 34 years, as this is the full extent of the NOAA OISST data available included in __`heatwaveR`__. The minimum length we will use is 3 years, as it is not possible to calculate the climatology statistics with fewer years than this. Any time series under 5 -- 10 years in length will almost certainly create wildly inaccurate results, so we use these short time series more as a proof of their usability than to show that they may be a viable option. 

Before we can discuss shortening techniques, we must first consider the inherent decadal trend in the time series themselves. This is usually the primary driver for much of the event detection changes over time [@Oliver2018]. Therefore, if we truly want to understand the effect that a shortened time series may have on event detection, apart from the effect of the decadal trend, we must de-trend the time series first. It is however worthwhile to compare results from the different time series lengths with and without the long-term trends removed. In order to do make the detected MHWs from different time series comparable, we propose below the use of two different shortening methodologies. These methods will allow us to address the question of how the length of a time series may affect the creation of the seasonal signal and 90th percentile threshold (hereafter referred to as "the climatologies"), and therefore the events detected. 

The first shortening methodology is to simply use the most recent _n_ years in a given time series (e.g. 3 -- 34) to observe events in that same period of time. This method will allow us to also directly compare results between the original and de-trended time series.

The second technique proposed here is [re-sampling](https://robwschlegel.github.io/MHWdetection/articles/Short_climatologies.html). This methodology proposes taking the full 34 year de-trended time series and randomly sampling _n_ years from it to simulate a 3 -- 34 year long time series. One then uses the jumbled up, randomly selected years of data to create the climatologies. The real temperature data, in the correct annual order, are still used against the re-sample created climatologies.

Re-sampling the time series in this way is useful because it allows us to replicate random climatology creation 100 (or more) times, in order to produce a more confident estimation of how likely climatologies generated from certain time series lengths are to impact the accuracy of event detection. This method however does not allow us to make direct comparisons of the effect of de-trending on observed events because the re-sampling will take years out of order so the long-term trend in the data will be random, and not something we can control for.

In order to better compare the re-sample climatologies, the following measurement metrics will be quantified:

 - for each day-of-year (doy) in the seasonal climatology and the 90th percentile threshold, calculate the SD of the 100 re-samples;
 - for each doy, calculate the RMSE of the re-sampled means relative to the true climatology (i.e. the one produced from the real (not re-sampled) 30-year long time series);
 - correspondence of detected events when using climatologies calculated from reduced time series vs. when using the full duration time series climatologies.

After looking at these effects, the next stage of this investigation will be to look into best practices on how to consistently detect events when time series are not of optimal length (> 30 years). 

First prize for all of this research would be to develop an equation (model) that could look at a time series and determine for the user how best to calculate the climatologies based on everything learned from the methodologies proposed above and whatever relationship(s) they may have with whatever may be found.


## Simply shorter

In this section we will compare the detection of events when we simply shorten them by removing the oldest years. This is presently being done with the three pre-packaged time series available in the __`heatwaveR`__ package and the python distribution, but eventually will be run on the global data.

```{r r-init}
library(tidyverse)
library(broom)
library(heatwaveR)
library(lubridate) # This is intentionally activated after data.table
library(fasttime)
library(ggpubr)
library(boot)
library(FNN)
library(mgcv)
library(doMC); doMC::registerDoMC(cores = 3)
```

The WMO recommends that the period used to create a climatology be 30 years in length, starting on the first year of a decade (e.g. 1981), however; because we are going to be running these analyses on time series of many different lengths, it will be better to use all of the years present in each as the climatology period. For example, if a time series spans 1989 -- 2014, that will be the period used for calculating the climatologies. Likewise should the time series only span the years 2006 -- 2014.

```{r prep-funcs}
# Remove the trend from a time series
detrend <- function(df){
  resids <- broom::augment(lm(temp ~ t, df))
  res <- df %>% 
    mutate(temp = temp - resids$.fitted)
  return(res)
}
# ggplot(sst_ALL_detrend, aes(x = t, y = temp)) +
#   geom_point() +
#   geom_smooth(method = "lm") +
#   facet_wrap(~site, nrow = 3)

# Calculate one specific clim period
ts2clm_sub <- function(year_index, df){
  res <- ts2clm(df, climatologyPeriod = c(paste0(year_index,"-01-01"), "2014-12-31")) %>% 
    filter(lubridate::year(t) >= year_index) %>% 
    mutate(year_index = year_index)
  return(res)
}

# Calculate the full range of clims in a time series
ts2clm_ALL <- function(df){
  seq_year <- seq(min(lubridate::year(df$t)), 
                  max(lubridate::year(df$t))-2)
  res <- plyr::ldply(seq_year, .fun = ts2clm_sub, .parallel = T, df = df)
  return(res)
}

# Calculate decadal trends
decadal_trend <- function(df) {
  df2 <- df %>% 
    ungroup() %>%
    mutate(date = floor_date(t, "month")) %>% 
    group_by(date) %>% 
    summarise(temp = mean(temp,na.rm = T)) %>% 
    mutate(year = year(date),
           num = seq(1, n()))
    dt <- round(as.numeric(coef(gls(
      temp ~ num, correlation = corARMA(form = ~ 1 | year, p = 2),
      method = "REML", data = df2, na.action = na.exclude))[2] * 120), 3)
  return(dt)
}
```

```{r shorten}
# First put all of the data together and create a site column
sst_ALL <- rbind(sst_Med, sst_NW_Atl, sst_WA) %>% 
  mutate(site = rep(c("Med", "NW_Atl", "WA"), each = 12053))

# Then create a de-trended version
sst_ALL_detrend <- sst_ALL %>% 
  group_by(site) %>% 
  do(detrend(.)) %>% 
  ungroup() %>% 
  as.tibble(.)

# Then calculate all of the clims
## With trend left in
sst_ALL_clim <- sst_ALL %>% 
  nest(-site) %>% 
  mutate(clim = map(data, ts2clm_ALL)) %>% 
  select(-data) %>% 
  unnest()
## With de-trended data
sst_ALL_detrend_clim <- sst_ALL_detrend %>% 
  nest(-site) %>% 
  mutate(clim = map(data, ts2clm_ALL)) %>% 
  select(-data) %>% 
  unnest()
## Combine for ease later
sst_ALL_both_clim <- rbind(sst_ALL_clim, sst_ALL_detrend_clim) %>% 
  mutate(trend = rep(c("normal", "detrended"), each = nrow(sst_ALL_clim)))

# Calculate the different decadal trends
# See Schlegel and Smit 2016 for GLS model explanation/validation
sst_ALL_DT <- sst_ALL_clim %>% 
  # group_by(site) %>% 
  nest(-site) %>% 
  mutate(DT = map(data, decadal_trend)) %>% 
  select(-data) %>% 
  unnest() #%>% 
  # rename(decades = decades2)
```
```{r shorten-table}
# Quick peak at the decadal trends
knitr::kable(sst_ALL_DT, col.names = c("site", "trend (°C/dec)"), align = "c", 
             caption = "Table showing the decadal trends detected for the three default sites.", 
             digits = 3)
```

We see in the table above that the three default time series currently being used all have decadal trends much higher than the global average of ~0.1°C.

```{r compare-funcs}
# A clomp of functions used below
# Written out here for tidiness/convenience

# Calculate only events
detect_event_event <- function(df, y = temp){
  ts_y <- eval(substitute(y), df)
  df$temp <- ts_y
  res <- detect_event(df)$event
  return(res)
  }

# Run an ANOVA on each metric of the combined event results and get the p-value
event_aov_p <- function(df){
  aov_models <- df[ , -grep("year_index", names(df))] %>%
    map(~ aov(.x ~ df$year_index)) %>% 
    map_dfr(~ broom::tidy(.), .id = 'metric') %>%
    mutate(p.value = round(p.value, 4)) %>%
    filter(term != "Residuals") %>%
    select(metric, p.value)
  return(aov_models)
  }

# Run an ANOVA on each metric and then a Tukey test
event_aov_tukey <- function(df){
  aov_tukey <- df[ , -grep("year_index", names(df))] %>%
    map(~ TukeyHSD(aov(.x ~ df$year_index))) %>% 
    map_dfr(~ broom::tidy(.), .id = 'metric') %>%
    mutate(p.value = round(adj.p.value, 4)) %>%
    # filter(term != "Residuals") %>%
    select(metric, comparison, adj.p.value) %>% 
    # filter(adj.p.value <= 0.05) %>% 
    arrange(metric, adj.p.value)
  return(aov_tukey)
  }

# Run an ANOVA on each metric of the combined event results and get CI

# df <- sst_ALL_both_event %>% 
#   filter(lubridate::year(date_peak) >= 2012, 
#          site == "WA",
#          trend == "detrended") %>% 
#   select(year_index, duration, intensity_mean, intensity_max, intensity_cumulative) #%>% 
  # select(site, trend, year_index, duration, intensity_mean, intensity_max, intensity_cumulative) #%>%   
  # nest(-site, -trend)

event_aov_CI <- function(df){
  label_levels <- unique(df$year_index)
  # Run ANOVAs
  # aov_models <- df[ , -grep("year_index", names(df))] %>%
  #   map(~ aov(.x ~ df$year_index)) %>% 
  #   map_dfr(~ confint_tidy(.), .id = 'metric') %>% 
  #   mutate(year_index = rep(label_levels, nrow(.)/length(label_levels))) %>% 
  #   select(metric, year_index, everything())
  df_conf <- gather(df, key = "metric", value = "value", -year_index) %>% 
    group_by(year_index, metric) %>% 
    summarise(error = qnorm(0.975)*sd(value)/sqrt(n()),
              lower = mean(value) - error,
              mid = mean(value),
              upper = mean(value) + error)
  # Calculate population means
  # df_mid <- df %>% 
  #   group_by(year_index) %>%
  #   summarise_all(.funs = mean) %>%
  #   gather(key = metric, value = conf.mid, -year_index)
  # Correct CI for first category
  # res <- aov_models %>%
  #   left_join(df_mean, by = c("metric", "year_index")) %>%
  #   mutate(conf.low = if_else(year_index == label_levels[1], conf.low - conf.mid, conf.low),
  #          conf.high = if_else(year_index == label_levels[1], conf.high - conf.mid, conf.high)) %>%
  #   select(-conf.mid)
  return(df_conf)
  }

# A particular summary output
event_output <- function(df){
  res <- df %>%
    group_by(year_index) %>% 
    # select(-event_no) %>% 
    summarise_all(c("mean", "sd"))
  return(res)
}

# Quick wrapper for getting results for ANOVA on clims
clim_aov <- function(df){
  res <- df %>% 
    select(-t, - temp, -doy) %>% 
    mutate(year_index = as.factor(year_index)) %>% 
    unique() %>% 
    group_by(site, trend) %>% 
    nest() %>% 
    mutate(res = map(data, event_aov_p)) %>% 
    select(-data) %>% 
    unnest()
  return(res)
}

# Quick wrapper for getting results for Tukey on clims
clim_tukey <- function(df){
  res <- df %>% 
    select(-t, - temp, -doy) %>% 
    mutate(year_index = as.factor(year_index)) %>% 
    unique() %>% 
    group_by(site, trend) %>% 
    nest() %>% 
    mutate(res = map(data, event_aov_tukey)) %>% 
    select(-data) %>% 
    unnest()
  return(res)
}
```


### Climatology statistics

#### ANOVA _p_-values

Given that there are perceptible differences in the mean seasonal signal values between the decades of data used, let's see if an ANOVA determines these differences to be significant.

```{r shorten-clim-aov, fig.cap="Heatmap showing the ANOVA results for the comparisons of the climatologies for the many different time periods for the default time series."}
sst_ALL_both_clim_aov <- clim_aov(sst_ALL_both_clim)

ggplot(sst_ALL_both_clim_aov, aes(x = site, y = metric)) +
  geom_tile(aes(fill = p.value)) +
  geom_text(aes(label = round(p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  facet_wrap(~trend)
```

We may see in the figure above that the climatologies do not differ appreciably at any length for the `Med`. The climatologies do however differ significantly for both the `NW_Atl` and the `WA` time series. Interestingly, when the long-term trend is removed (simple linear model flattening), the 90th percentile thresholds do not differ significantly at any length within the `NW_Atl` time series. It is worth pointing out that the `Med` time series has the lowest decadal warming trend of the three time series, that coupled with the decrease in significance for the difference in thresholds in the `NW_Atl` seems to imply that the decadal trend may play a role in determining the difference in the time series. Though the fact that the `WA` time series, by far the most variable of the three, always produces significant differences leads me to think that the variance of a time series may be able to win out over the long-term trend. This interaction will need to be quantified.


#### Post-hoc Tukey test

Before getting to that step we need to run a post-hoc Tukey test to see where the significant differences when comparing the different time series lengths begins.

```{r shorten-tukey, fig.cap="Heatmap showing the Tukey test results from the ANOVAs run on the climatologies generated from all possible time series lengths (i.e. 3 -- 34 years). The different sites are shown as different rows of panels, and the different climatologies (seasonal signal and 90th percentile threshold, from both normal and detrended time series) are shown as different columns. The X and Y axes here denote two different start years for time series being compared. For example, a pixel at '1990' on the X axis, and '2010' on the Y axis is showing the difference (_p_-value) between the climatology generated from a time series from 1990 -- 2014 against the climatology from the same time series that runs only from 2010 -- 2014. Significant differences are marked with a small black dot. Note that results tend not to differ significantly once the time series being compared are more than several years long."}
sst_ALL_both_clim_tukey <- clim_tukey(sst_ALL_both_clim) %>% 
  tidyr::separate(col = comparison, into = c("comp1", "comp2")) %>% 
  mutate(comp1 = as.numeric(comp1), 
         comp2 = as.numeric(comp2))

ggplot(sst_ALL_both_clim_tukey, aes(x = comp2, y = comp1)) +
  geom_tile(aes(fill = adj.p.value)) +
  geom_point(data = filter(sst_ALL_both_clim_tukey, adj.p.value <= 0.05),
             size = 0.01) +
  # geom_text(aes(label = round(adj.p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  coord_equal(expand = 0) +
  facet_grid(site~metric+trend) +
  labs(x = "", y = "")
```

The figure above is perhaps a bit odd, but I think a lengthy table would have been less informative. What the figure above is showing is that most time series lengths do not create significantly different results for the calculated climatologies. We also see that even with very short time series (< 10 years), the calculated climatologies tend not to differ from those created from nearby years. The exception to this is the `WA` time series. We may see that the 90th percentile threshold for the three year time series (top row of pixels in the `WA` panels) differs significantly from the ~4 -- 7 year time series, but then does not differ significantly from the much longer time series. This is certainly due to the massive MHW near the end of this time series. It is a good indicator of one of the perils of using such short time series; that one massive event can create significant changes in the results. 


#### Kolmogorov-Smirnov tests

Now knowing that the daily values that make up the climatologies tend to differ based on the number of years used, as seen with the ANOVA and Tukey results, we want to check where the distributions of the climatologies themselves begin to differ. We will do this through a series of pair-wise two-sample KS tests.

```{r KS-clims}
# Extract climatology values only
sst_ALL_both_clim_only <- sst_ALL_both_clim %>% 
  select(-t, -temp) %>% 
  unique() %>% 
  arrange(site, trend, year_index, doy)

KS_sub <- function(df, year_index_1, year_index_2){
  df_1 <- df %>% 
    filter(year_index == year_index_1)
  df_2 <- df %>% 
    filter(year_index == year_index_2)
  res <- data.frame(seas = round(ks.test(df_1$seas, df_2$seas)$p.value, 4),
                    thresh = round(ks.test(df_1$thresh, df_2$thresh)$p.value, 4),
                    year_1 = year_index_1,
                    year_2 = year_index_2)
  return(res)
}

# The KS results
sst_ALL_both_clim_KS_p <- data.frame()
for(i in 1:length(unique(sst_ALL_both_clim_only$year_index))){
  year_index_1 <- unique(sst_ALL_both_clim_only$year_index)[i]
  for(j in 1:length(unique(sst_ALL_both_clim_only$year_index))){
    year_index_2 <- unique(sst_ALL_both_clim_only$year_index)[j]
    if(year_index_1 < year_index_2){
      suppressWarnings(
      res <- sst_ALL_both_clim_only %>% 
        group_by(site, trend) %>% 
        do(KS_sub(., year_index_1 = year_index_1, year_index_2 = year_index_2)) %>% 
        ungroup() %>% 
        as.tibble(.)
      )
      sst_ALL_both_clim_KS_p <- rbind(sst_ALL_both_clim_KS_p, res)
    }
  }
}

sst_ALL_both_clim_KS_p_long <- sst_ALL_both_clim_KS_p %>% 
  tidyr::gather(key = "metric", value = "p.value", -site, -trend, -year_1, -year_2)

ggplot(sst_ALL_both_clim_KS_p_long, aes(x = year_1, y = year_2)) +
  geom_tile(aes(fill = p.value)) +
  geom_point(data = filter(sst_ALL_both_clim_KS_p_long, p.value <= 0.05),
             size = 0.01) +
  # geom_text(aes(label = round(adj.p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  coord_equal(expand = 0) +
  facet_grid(site~metric+trend) +
  labs(x = "", y = "")
```

The table above shows that while climatology values only generated from short time series against long time series tend to be significantly different when tested with an ANOVA, Kolmogorov-Smirnov tests on the shape of these distributions say otherwise. For the `WA` time series, nearly any comparison being made will show that the climatology distributions are significantly different. However, we also see different patterns in each of the other two time series. These results do not show one clear pattern and so one is left to wonder how representative these results are. This will require that this analysis be run on a global scale as well.


### MHW metrics

#### ANOVA

With the effects of shortened time series on the calculation of climatologies quantified, we will now compare the results of the MHWs detected using an ANOVA. Unfortunately, in order to make a direct comparison of all of the MHW detected in the different time series lengths, we may only use the date range for the shortest time series used. This means that only events whose peak dates occurred in 2012 -- 2014 will be used.

```{r compare-shorten}
# Calculate events and filter only those from 2002 -- 2011
sst_ALL_both_event <- sst_ALL_both_clim %>% 
  group_by(site, trend, year_index) %>% 
  nest() %>% 
  mutate(res = map(data, detect_event_event)) %>% 
  select(-data) %>% 
  unnest(res) #%>% 
  # filter(date_start >= "2002-01-01", date_end <= "2011-12-31") %>% 
  # select(-c(index_start:index_end, date_start:date_end))
```
```{r compare-shorten-p, fig.cap="Heatmap showing the ANOVA results for the comparisons of the main four MHW metrics from events that peaked in the years 2012 -- 2014 from all time series lengths, both with and without the long-term trend removed."}
# ANOVA p
sst_ALL_both_event_aov_p <- sst_ALL_both_event %>% 
  filter(lubridate::year(date_peak) >= 2012) %>% 
  select(site, trend, year_index, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  nest(-site, -trend) %>%
  mutate(res = map(data, event_aov_p)) %>% 
  select(-data) %>% 
  unnest() #%>% 
  # spread(key = metric, value = p.value)

# visualise
ggplot(sst_ALL_both_event_aov_p, aes(x = site, y = metric)) +
  geom_tile(aes(fill = p.value)) +
  geom_text(aes(label = round(p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  theme(axis.text.y = element_text(angle = 75, hjust = 0.5)) +
  facet_wrap(~trend)
```

The heatmap above shows us that the removal of the long-term trend in the time series often has a significant impact on how much the detected MHWs may differ depending on time series length. Curiously, detrending the time series actually makes the differences between the max and mean intensities not significant for the `NW_Atl` time series. The opposite is true for the other two time series. What's also interesting is that when the trend is left in, none of the MHW metrics for events detected in the `Med` differ significantly, but the removal of the trend causes all of them to become significantly different.


#### Post-hoc Tukey test

It should not be surprising that comparing events detected from a climatology based on three years of data may differ significantly from those based on 34 years of data. Where the differences emerge is the real question. And to find this out we will use a post-hoc TUkey test, as done for the climatologies above.

```{r shorten-event-tukey, fig.cap="Heatmap showing the Tukey test results from the ANOVAs run on the MHW metrics generated from all possible time series lengths (i.e. 3 -- 34 years). The different sites are shown as different rows of panels, and the different climatologies (seasonal signal and 90th percentile threshold, from both normal and detrended time series) are shown as different columns. The X and Y axes here denote two different start years for time series being compared. For example, a pixel at '1990' on the X axis, and '2010' on the Y axis is showing the difference (_p_-value) between the climatology generated from a time series from 1990 -- 2014 against the climatology from the same time series that runs only from 2010 -- 2014. Significant differences are marked with a small black dot."}

sst_ALL_both_event_tukey <- sst_ALL_both_event %>%
  filter(lubridate::year(date_peak) >= 2012) %>% 
  select(site, trend, year_index, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  mutate(year_index = as.factor(year_index)) %>% 
  group_by(site, trend) %>% 
  nest() %>% 
  mutate(res = map(data, event_aov_tukey)) %>% 
  select(-data) %>% 
  unnest() %>% 
  tidyr::separate(col = comparison, into = c("comp1", "comp2")) %>% 
  mutate(comp1 = as.numeric(comp1), 
         comp2 = as.numeric(comp2))

ggplot(sst_ALL_both_event_tukey, aes(x = comp2, y = comp1)) +
  geom_tile(aes(fill = adj.p.value)) +
  geom_point(data = filter(sst_ALL_both_event_tukey, adj.p.value <= 0.05),
             size = 0.01) +
  # geom_text(aes(label = round(adj.p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  coord_equal(expand = 0) +
  facet_grid(site~metric+trend) +
  labs(x = "", y = "")
```

The results of the Tukey tests shown above demonstrate that, except for one comparison, the differences between the events detected with different time series lengths do not differ significantly on a pairwise basis. There are however some interesting artifacts in the results that seem to show that some dramatic events may be having a large impact on how the results compare against one another.


#### Confidence intervals

```{r event-CI-plot1, fig.cap="Confidence intervals of the different metrics for each of the shortened time series. Each panel shows the results for one of four MHW metrics. The X axis shows which site the CI bars are representing, and the Y axis shows the spread of the confidence around the population mean of the MHW metric."}
# ANOVA CI
sst_ALL_both_event_CI <- sst_ALL_both_event %>% 
  filter(lubridate::year(date_peak) >= 2012) %>% 
  select(site, trend, year_index, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  nest(-site, -trend) %>%
  mutate(res = map(data, event_aov_CI)) %>% 
  select(-data) %>% 
  unnest()

CI_plot_1 <- ggplot(sst_ALL_both_event_CI, aes(x = year_index)) +
  geom_errorbar(position = position_dodge(0.9), width = 0.5,
                aes(ymin = lower, ymax = upper)) +
  geom_point(aes(y = mid), colour = "red", size = 0.5) +
  facet_grid(metric+trend~site, scales = "free_y")
CI_plot_1
```

When we look at the confidence intervals (CI) we see that all of the MHW metrics for all of the time periods overlap rather well, with the exception of the most recent few years in `WA`. We also see that the duration/intensity of MHWs tend to increase for the first few years of data being added on, but that this flattens out around ~ 7 -- 10 years. This effect is more pronounced in the time series that have not had their long-term trends removed.


#### Median differences

(ECJO)[The results of this analysis will be highly sensitive to the underlying trend in the data. In fact that is what will be driving the differences, on average.
Therefore, this really has nothing to do with time series length, and probably shouldn't form part of the paper.]
(RWS)[Yes... This paper is suffering from an identity crisis and I need to think about how to resolve it.]

Now that we know that the detected events do not differ significantly when different lengths are used, we still want to know by what amounts they do differ. This information will then later be used during the re-sampling to see if this gap can be reduced through the clever use of statistics.

```{r event-box1, fig.cap="Boxplots showing the spread of the metric values for the events detected in the most recent decade with the three different time periods."}
# Long data.frame for plotting
sst_ALL_both_event_long <- sst_ALL_both_event %>% 
  select(site, trend, year_index, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  # select(-event_no) %>%
  gather(variable, value, -site, -trend, - year_index)

# Visualisations
ggplot(sst_ALL_both_event_long, aes(x = as.factor(year_index), y = value)) +
  geom_boxplot(aes(fill = year_index), position = "dodge") +
  facet_grid(variable+trend~site, scales = "free_y")
```

```{r event-median-table}
sst_ALL_both_event_median <- sst_ALL_both_event_long %>% 
  group_by(site, trend, year_index, variable) %>% 
  summarise(value = median(value))

spread(sst_ALL_both_event_median, key = variable, value = value) %>% 
  arrange(site, trend, rev(year_index)) %>% 
  knitr::kable(digits = 2, 
               col.names = c("site", "trend", "start year", "duration", "int. cum.", "int. max", "int. mean"),
               caption = "Median values for the MHW metrics for the three different time series sites and lengths.")
```


The Mediterranean (`Med`) data change over time in a more predictable, linear fashion. The Western Australia (`WA`) data changed very little depending on the number of decades used for the climatology, which is most certainly due to that one huge event near the end. The Northwest Atlantic (`NW_Atl`) results for the 10 and 20 year periods appear to generally be more similar than for the 30 year period.

I think the take away message from this is that one cannot be certain how time series length may affect the MHW metrics as with three different time series we see three different patterns. This must mean that there is a more complex relationship somewhere else within the time series that is affecting these values. But what that is must be important as the median duration for Mediterranean events from 30 year time series is 50% longer than from the 10 year time series.

With these benchmarks established, we will now move on to re-sampling to see if the effect of a shorter time series on the detected climatology can be mitigated.


## Re-sampling

With the effect of shortening time series on the detection of events quantified, we will now perform [re-sampling](https://robwschlegel.github.io/MHWdetection/articles/Short_climatologies.html) to simulate one hundred 10, 20, and 30 year time series in order to quantify how much more variance one may expect from shorter time series. This will be measured through the following statistics:

 * for each day-of-year (doy) in the climatology, calculate the SD of the climatological means of the 100 re-samplings;
 * for each doy, calculate the RMSE of the re-sampled means relative to the true climatology (i.e. the one produced from the 30-year long time series);
 * correspondence of detected events when using climatologies calculated from reduced time series vs. when using the full duration time series climatologies.

The secondary goal of this step in this section of the methodology is to also identify how much more accurate this re-sampling may be able to make the climatologies generated from shorter time series as a technique for addressing this potential short-coming (yes, that was a pun).

```{r re-sample-func}
### Need to insert here a function that does what 'sample_n' does
### But only selects from the appropriate range of dates

sst_repl <- function(sst) {
  sst.sampled <- sst %>% 
    mutate(sample_10 = map(data, sample_n, 10, replace = TRUE),
           sample_20 = map(data, sample_n, 20, replace = TRUE),
           sample_30 = map(data, sample_n, 30, replace = TRUE))
  return(sst.sampled)
}

parse_date_sst <- function(data, rep_col, len) {
  parsed <- data %>% 
    mutate(id = rep_col,
           y = year(t),
           m = month(t),
           d = day(t)) %>% 
    group_by(site, rep, doy) %>% 
    mutate(y = seq(2012-len, by = 1, len = len)) %>% 
    mutate(t = as.Date(fastPOSIXct(paste(y, m, d, sep = "-")))) %>%
    select(-y, -m, -d) %>% 
    na.omit() # because of complications due to leap years
  return(parsed)
}
```

```{r re-sample-clims, eval = FALSE}
sst_ALL_doy <- sst_ALL %>%
  mutate(doy = yday(as.Date(t))) %>%
  nest(-site, -doy)

sst_ALL_repl <- purrr::rerun(100, sst_repl(sst_ALL_doy)) %>%
  map_df(as.data.frame, .id = "rep")

sample_10 <- sst_ALL_repl %>%
  unnest(sample_10) %>%
  parse_date_sst("sample_10", len = 10) %>%
  group_by(site, id, rep) %>%
  nest() %>%
  mutate(smoothed = map(data, function(x) ts2clm(x, climatologyPeriod = c("2002-01-01", "2011-12-31")))) %>%
  unnest(smoothed)

sample_20 <- sst_ALL_repl %>%
  unnest(sample_20) %>%
  parse_date_sst("sample_20", len = 20) %>%
  group_by(site, id, rep) %>%
  nest() %>%
  mutate(smoothed = map(data, function(x) ts2clm(x, climatologyPeriod = c("1992-01-01", "2011-12-31")))) %>%
  unnest(smoothed)

sample_30 <- sst_ALL_repl %>%
  unnest(sample_30) %>%
  parse_date_sst("sample_30", len = 30) %>%
  group_by(site, id, rep) %>%
  nest() %>%
  mutate(smoothed = map(data, function(x) ts2clm(x, climatologyPeriod = c("1982-01-01", "2011-12-31")))) %>%
  unnest(smoothed)

sst_ALL_smooth <- bind_rows(sample_10, sample_20, sample_30)
save(sst_ALL_smooth, file = "data/sst_ALL_smooth.Rdata")
```

```{r re-sample-clims-load}
# This file is not uploaded to GitHub as it is too large
# One must first run the above code locally to generate and save the file
load("data/sst_ALL_smooth.Rdata")
```

### Climatology statistics

#### ANOVA _p_-values

With 100 re-sampled climatologies and thresholds created, let's take a moment to see if this much larger sample size differs significantly. Keep in mind however that the years used to create these re-sampled time series come from the pool of the total 34 years, and so the climatologies created from 10 year re-samplings are not going to necessarily be warmer than from 20 or 30 year re-samplings.

```{r re-sample-clim-aov, fig.cap="Heatmap showing the _p_-values from ANOVA's for the three different re-sampling lengths."}
sst_ALL_smooth_aov <- sst_ALL_smooth %>% 
  rename(decades = id) %>% 
  select(-rep, -t, -temp, -doy) %>% 
  mutate(decades = as.factor(decades)) %>% 
  unique() %>% 
  group_by(site) %>% 
  nest() %>% 
  mutate(res = map(data, event_aov_p)) %>% 
  select(-data) %>% 
  unnest()

ggplot(sst_ALL_smooth_aov, aes(x = site, y = metric)) +
  geom_tile(aes(fill = p.value)) +
  geom_text(aes(label = round(p.value, 4))) +
  scale_fill_gradient2(midpoint = 0.1)
```

The figure above shows what we have been seeing consistently. The variance values differ significantly, but the climatologies do not. 


#### Kolmogorov-Smirnov tests

```{r KS-smooth, fig.cap="Histograms showing the _p_-value results from Kolmogorov-Smirnov tests comparing the distributions of each of the 100 re-sampled climatologies against the true (non-re-sampled) climatologies for the same time series length for each of the three sites."}
# KS function for re-sampled data
smooth_KS_p <- function(df){
  df_sub <- sst_ALL_clim_only %>% 
    filter(site == df$site[1])
  if(df$decades[1] == "sample_10"){
    df_comp <- df_sub %>% 
      filter(decades == "clim_10")
  } else if(df$decades[1] == "sample_20") {
    df_comp <- df_sub %>% 
      filter(decades == "clim_20")
  } else if(df$decades[1] == "sample_30") {
    df_comp <- df_sub %>% 
      filter(decades == "clim_30")
  }
  res <- data.frame(site = df$site[1],
                    decades = df$decades[1],
                    seas = ks.test(df$seas, df_comp$seas)$p.value,
                    thresh = ks.test(df$thresh, df_comp$thresh)$p.value,
                    var = ks.test(df$var, df_comp$var)$p.value)
  return(res)
  }

sst_ALL_smooth_KS_p <- sst_ALL_smooth %>% 
  select(-t, -temp, -doy) %>% 
  rename(decades = id) %>% 
  mutate(decades = as.factor(decades)) %>% 
  unique() %>% 
  mutate(site_i = site,
         decades_i = decades) %>% 
  group_by(site_i, decades_i,  rep) %>% 
  nest() %>% 
  mutate(res = map(data, smooth_KS_p)) %>% 
  select(-data) %>% 
  unnest() %>% 
  select(site, decades, rep, seas:var) %>% 
  gather(key = stat, value = p.value, -site, -decades, -rep)

# visualise
ggplot(sst_ALL_smooth_KS_p, aes(x = p.value, fill = site)) +
  geom_histogram(bins = 20) +
  facet_grid(stat~decades)
```
 
We may see in the figure above that the distributions of the variance are significantly different from the real (non-re-sampled) distributions regardless of how many re-samples are run. That is somewhat surprising. We also see that 10 and 20 years of re-sampling tend to produce significantly different climatologies, but that at 30 years of re-sampling the results come more in line with reality. The exception here is that the climatologies for Mediterranean seem to happily be able to be reproduced with only 10 years of re-sampling. I think this is because it has a smaller decadal trend than the other two time series as well as less inter-annual variability. But this needs to be more thoroughly quantified.


#### doy SD

(ECJO)[I have a hard time seeing what the doy std. dev. and RMSE are telling us. Is it information contributing to the question we are asking?  I don’t think so….]
(RWS)[This was incorporated at the outset of the project, but I'm not certain it is still the focus of the paper. It may get cut when I re-think what is the central question we are trying to answer.]

```{r re-sample-stats-sd, fig.cap="Line plot showing the standard deviation (sd) of each day of the year (doy) for the three different sites (columns) and the different climatologies (rows). The line colours denote the number of samples used for each doy. All re-samples were run 100 times, thus n = 100 for each sd of each doy for each id."}
# for each day-of-year (doy) in the climatology, calculate the SD of the climatological means of the 100 re-samplings;
sst_ALL_sd <- sst_ALL_smooth %>% 
  group_by(site, id, doy) %>% 
  summarise(seas_sd = sd(seas),
            thresh_sd = sd(thresh),
            var_sd = sd(var)) %>% 
  gather(variable, value, -site, -id, -doy)

ggplot(sst_ALL_sd, aes(x = doy, y = value)) +
  geom_line(aes(colour = id)) +
  facet_grid(variable ~ site)
```

Hmmm.... Interesting how the seasonal and threshold signals of increased summer variance comes through so nicely in the `Med` data the fewer samples are taken. The other two time series produce somewhat strange signals. We can see that the 30 year, and to a lesser extent 20 year, re-sampled time series are much smoother than the 10 year re-sample. The `WA` data are either very strange, or very exposed to particularly intense events in the middle of summer.


#### doy RMSE

```{r  re-sample-stats-RMSE, fig.cap="Line graph, as above, but now showing the RMSE for each doy for the three different climatology durations."}
# Base 30 year clims
sst_ALL_clim_base <- sst_ALL_clim %>% 
  filter(decades == "clim_30") %>% 
  select(site:doy, seas:var, -decades) %>% 
  unique()

# For each doy, calculate the RMSE of the re-sampled means relative to the true climatology (i.e. the one produced from the 30-year long time series);
sst_ALL_error <- sst_ALL_smooth %>% 
  select(site:doy, seas:var) %>% 
  unique() %>% 
  left_join(sst_ALL_clim_base, by = c("site", "doy")) %>% 
  mutate(seas.er = seas.x - seas.y,
         thresh.er = thresh.x - thresh.y,
         var.er = var.x - var.y)

sst_ALL_RMSE <- sst_ALL_error %>% 
  group_by(site, id, doy) %>% 
  summarise(seas_RMSE = sqrt(mean(seas.er^2)),
            thresh_RMSE = sqrt(mean(thresh.er^2)),
            var_RMSE = sqrt(mean(var.er^2))) %>% 
  gather(variable, value, -site, -id, -doy)

ggplot(sst_ALL_RMSE, aes(x = doy, y = value)) +
  geom_line(aes(colour = id)) +
  facet_grid(variable ~ site)
```

As with the SD values in the previous figure, we see in the RMSE figure above that 10 years of re-sampling produce noticeably larger errors than the rather similar 20 and 30 year re-samples. That being said, the differences aren't large.


### MHW metrics

#### ANOVA

```{r re-sample-stats-temp-swap}
# It is necessary to replace the 'temp' values in the smoothed data with the real values
sst_ALL_smooth_real <- sst_ALL_smooth %>% 
  select(-temp) %>% 
  left_join(sst_ALL, by = c("site", "t"))
```

```{r re-sample-stats-event-calc, eval=FALSE}
# Then caluclate events using the many re-smapled clims on the real temperature data
sst_ALL_smooth_event <- sst_ALL_smooth_real %>% 
  group_by(site, id, rep) %>% 
  nest() %>% 
  mutate(res = map(data, detect_event_event)) %>%
  select(-data) %>% 
  unnest(res) %>% 
  filter(date_start >= "2002-01-01", date_end <= "2011-12-31") %>% 
  select(-c(index_start:index_end, date_start:date_end))

save(sst_ALL_smooth_event, file = "data/sst_ALL_smooth_event.Rdata")
```

```{r re-sample-stats-event-calc-load}
# This file is not uploaded to GitHub as it is too large
# One must first run the above code locally to generate and save the file
load("data/sst_ALL_smooth_event.Rdata")

# Rename for project-wide consistency
sst_ALL_smooth_event <- sst_ALL_smooth_event %>% 
  rename(decades = id)
```

```{r re-sample-event-p, fig.cap="Heatmap showing the ANOVA results (_p_-values) when comparing the events detected based on climatologies created from 10, 20, or 30 years of re-sample data."}
# ANOVA p
sst_ALL_smooth_aov_p <- sst_ALL_smooth_event %>% 
  select(site, decades, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  nest(-site) %>%
  mutate(res = map(data, event_aov_p)) %>% 
  select(-data) %>% 
  unnest() #%>% 
  # spread(key = metric, value = p.value) %>%
  # select(names(select(sst_ALL_event, -decades, -event_no)))
# sst_ALL_smooth_aov_p

# visualise
ggplot(sst_ALL_smooth_aov_p, aes(x = site, y = metric)) +
  geom_tile(aes(fill = p.value)) +
  geom_text(aes(label = round(p.value, 2))) +
  scale_fill_gradient2(midpoint = 0.1) +
  theme(axis.text.y = element_text(angle = 90, hjust = 0.5))
```

The heatmap above shows the ANOVA results (_p_-values) when one compares the events detected in the 100 replicated re-samples of 10, 20, or 30 years of data. Meaning that these are very large pools of events being compared against one another.

(RWS: It would be worthwhile to compare the MHW metrics for each individual re-sampled time series against the MHW metrics from the real time series of the same length.)


#### Confidence intervals

```{r event-CI-plot2, fig.cap="Confidence intervals of the different metrics for the three different clim periods from the population mean based on the 100 times re-sampling of each clim period in red, and the single sample based on the real data in black."}
# ANOVA CI
sst_ALL_smooth_aov_CI <- sst_ALL_smooth_event %>% 
  select(site, decades, duration, intensity_mean, intensity_max, intensity_cumulative) %>% 
  nest(-site) %>%
  mutate(res = map(data, event_aov_CI)) %>% 
  select(-data) %>% 
  unnest()

# Plot
ggplot(sst_ALL_aov_CI, aes(x = site)) +
  geom_errorbar(position = position_dodge(0.9), width = 0.5, colour = "black",
                aes(ymin = conf.low, ymax = conf.high, linetype = decades)) +
  geom_errorbar(data = sst_ALL_smooth_aov_CI, show.legend = F,
                position = position_dodge(0.9), width = 0.5, colour = "red",
                aes(ymin = conf.low, ymax = conf.high, linetype = decades)) +
  facet_wrap(~metric, scales = "free_y")
```

The CI plot above demonstrates that the general increase in MHW metrics in longer time series detected in the first round of experiments (when the real 10, 20, and 30 year baselines were used to generate a single clim each) does not hold up when we re-sample the data 100 times. The centre around which the CI spread shrinks dramatically with re-sampling, but we also see how the three different time periods converge around 0, whereas the samples from  the real data show the effect that the long-term trend has in the calculation of the metrics. The reason that we do not see the trend of increasing MHW duration/intensity with the re-sampling is that re-sampling effectively removes the decadal trend from the data. It is for this reason that I recommended at the start of this document that a second type of re-sampling be performed that would better maintain the effect that the decadal trend has on the data.

Note that in an earlier version of this vignette bootstrapping was also tested. It has since been removed as it was shown to not be effective. This was because the bootstrapping of random values to create climatologies created much lower values than the real data because while the bootstrapping does sample the data randomly, it then takes those _n_ random samples and creates one mean value from them. This then makes artificially even values and so when one wants to calculate a 90th percentile threshold from this it is almost identical to the seasonal climatology.


#### Median difference between decades

(RWS: I've not calculated the median differences for the re-sampled data as one may see that they are practically identical.)


## Categories

In this section we want to look at how the categories in the different time periods compare. I'll start out with doing basic calculations and comparisons of the categories of events with the different time periods. And then based on how that looks, see if I can think of a way of quantifying the differences.

```{r category-event-func}
# Calculate categories from events in one function for nesting
detect_event_category <- function(df, y = temp){
  ts_y <- eval(substitute(y), df)
  df$temp <- ts_y
  res <- category(detect_event(df))
  res$event_name <- as.character(res$event_name)
  return(res)
  }
```

```{r clim-category-calc}
# Calculate categories and filter only those from 2002 -- 2011
sst_ALL_clim_category <- sst_ALL_clim %>% 
  group_by(site, decades) %>% 
  nest() %>% 
  mutate(res = map(data, detect_event_category)) %>% 
  select(-data) %>% 
  unnest(res) %>% 
  filter(peak_date >= "2002-01-01", peak_date <= "2011-12-31") #%>% 
  # select(-c(index_start:index_end, date_start:date_end))
```

```{r clim-category-compare, fig.cap="Bar plots showing the counts of the different categories of events faceted in a grid with different sites along the top and the different category classifications down the side. The colours of the bars denote the different climatology periods used. Note that the general trend is that more 'smaller' events are detected with a 10 year clim period, and more 'larger' events detected with the 30 year clim. The 20 year clim tends to rest in the middle. Note that these are from the real data, not the re-sampled data."}
# Indeed... but how to compare qualitative data?
# Let's start out by counting the occurrence of the categories
sst_ALL_clim_category_count <- sst_ALL_clim_category %>% 
  group_by(site, decades, category) %>% 
  summarise(count = n()) %>% 
  mutate(category = factor(category,
                           levels = c("I Moderate", "II Strong",
                                      "III Severe", "IV Extreme")))

ggplot(sst_ALL_clim_category_count, aes(x = decades, y = count, fill = decades)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = count, y = count/2)) +
  facet_grid(category~site)
```

The results in the figure above are about what I was expecting. We tend to see more larger events when the climatologies are based on 30 years of data, rather than 10.

```{r re-sample-category-calc, eval=FALSE}
# Calculate categories and filter only those from 2002 -- 2011
sst_ALL_smooth_real_category <- sst_ALL_smooth_real %>% 
  group_by(site, id, rep) %>% 
  nest() %>% 
  mutate(res = map(data, detect_event_category)) %>% 
  select(-data) %>% 
  unnest(res) %>% 
  filter(peak_date >= "2002-01-01", peak_date <= "2011-12-31") #%>% 
  # select(-c(index_start:index_end, date_start:date_end))
save(sst_ALL_smooth_real_category, file = "data/sst_ALL_smooth_real_category.Rdata")
```

```{r re-sample-category-calc-load}
load("data/sst_ALL_smooth_real_category.Rdata")
```

```{r re-sample-category-compare, fig.cap="Bar plots showing the counts of the different categories of events from 100 re-samplings at the three different lengths (10, 20, and 30 years). The faceted grid shows different sites along the top and the different category classifications down the side. The colours of the bars denote the different climatology periods used. There is no real difference between any of the time periods. Note that these values are the average for all 100 re-samplings for each time periods per site, rounded to the nearest whole number."}
# Indeed... but how to compare qualitative data?
# Let's start out by counting the occurrence of the categories
sst_ALL_smooth_real_category_count <- sst_ALL_smooth_real_category %>% 
  rename(decades = id) %>% 
  group_by(site, decades, category) %>% 
  summarise(count = n()/100) %>% 
  mutate(category = factor(category,
                           levels = c("I Moderate", "II Strong",
                                      "III Severe", "IV Extreme")))

ggplot(sst_ALL_smooth_real_category_count, aes(x = decades, y = count, fill = decades)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = round(count), y = count/2)) +
  facet_grid(category~site)
```

When re-sampling we actually see the detection of *more* larger category events with the 10 year periods than with either 20 or 30, but this is negligible. As noted earlier, this re-sampling removes the decadal trend from the data, which seems to cause the climatologies to become more similar, not less. Remember that the real temperature values are being used to detect these events, not the re-sampled temperature values that were used to calculate the climatologies. This was because the re-sampled temperatures were too jumbled to detect events with reliably.


### Comparing top events

(AJS: I was thinking about the event categories. What we need to do is detect the top five events using the 30 yr climatology. Then, using the sections of time series that overlap with these events, use reduced time series, make climatologies, and see how those top five events compare in their event metrics to those detected using the shorter climatologies. Then we do the reverse. Detect events in the reduced time series, find the top five, and see what the matching ones in the full time series are like in comparison. So, compare specific events in addition to the way in which you have done it. Then do the same for median-sized events, and the smallest ones. Maybe even those at the 2nd and 5th quantiles.)

(RWS: Parts of this proposed methodology proved to be rather tricky given how the rest of the methodology has been framed thus far so I have not yet made all of the suggestion posed here. I have only compared the top five events, as proposed by AJS above.)

(ECJO)[All of this section is really just pulling out the effect of the long-term trend.  It is not quantifying the uncertainty due to a short time series.]

(RWS)[This again is the same problem running throughout. It would perhaps be best just to detrend everything...]

```{r}
# Calculate all event categories, not just the last ten years as done above
sst_ALL_clim_category_ALL <- sst_ALL_clim %>% 
  group_by(site, decades) %>% 
  nest() %>% 
  mutate(res = map(data, detect_event_category)) %>% 
  select(-data) %>% 
  unnest(res)

# The top 5 events froom 30 years
sst_ALL_clim_category_30_top_5 <- sst_ALL_clim_category_ALL %>% 
  filter(decades == "clim_30",
         peak_date >= as.Date("1982-01-01"),
         peak_date <= as.Date("2011-12-31")) %>% 
  group_by(site) %>% 
  arrange(-i_max) %>% 
  slice(1:5)

# The top 5 events froom 20 years
sst_ALL_clim_category_20_top_5 <- sst_ALL_clim_category_ALL %>% 
  filter(decades == "clim_20",
         peak_date >= as.Date("1992-01-01"),
         peak_date <= as.Date("2011-12-31")) %>% 
  group_by(site) %>% 
  arrange(-i_max) %>% 
  slice(1:5)

# The top 5 events froom 10 years
sst_ALL_clim_category_10_top_5 <- sst_ALL_clim_category_ALL %>% 
  filter(decades == "clim_10",
         peak_date >= as.Date("2002-01-01"),
         peak_date <= as.Date("2011-12-31")) %>% 
  group_by(site) %>% 
  arrange(-i_max) %>% 
  slice(1:5)
```

```{r}
# RWS: This is a bit of a schlep as it requires a much more nuanced take on how to go about comparing relavent lengths of clim periods. The method used throughout here, that of the most recent 10, 20, or 30 years for the clim period won't work here and I hesitate to create a new methodology this late in the section...
# RWS: So for now I'm skipping forward to the next step as that will work with the current methodology.
```

```{r}
# Function for pulling out comparable events based on closest peak dates
peak_date_index <- function(df){
  sst_30 <- sst_ALL_clim_category_ALL %>% 
    filter(decades == "clim_30")
  peak_index <- as.data.frame(knnx.index(as.matrix(sst_30$peak_date), 
                                         as.matrix(df$peak_date), k = 1))
  res <- sst_30[as.matrix(peak_index),]
}
```

```{r}
# Find matching events based on nearest peak_date
sst_ALL_clim_category_10_top_5_match <- peak_date_index(sst_ALL_clim_category_10_top_5)
sst_ALL_clim_category_20_top_5_match <- peak_date_index(sst_ALL_clim_category_20_top_5)
```

```{r category_30_vs_all, fig.cap="Bar plot showing the categories of events from the 30 year climatology calculations when they are determined by the top five events in the shorter time series. This isn't terribly informative as it is effectively showing if larger events happened earlier on in the time series or not. It says little about how the climatology period itself affects the categories of events."}
sst_ALL_clim_category_top_5_match <- rbind(data.frame(sst_ALL_clim_category_10_top_5_match, match = "clim_10"),
                                           data.frame(sst_ALL_clim_category_20_top_5_match, match = "clim_20"),
                                           data.frame(sst_ALL_clim_category_30_top_5, match = "clim_30")) %>% 
  group_by(site, match, category) %>% 
  summarise(count = n()) %>% 
  mutate(category = factor(category,
                           levels = c("I Moderate", "II Strong",
                                      "III Severe", "IV Extreme")))

ggplot(sst_ALL_clim_category_top_5_match, aes(x = match, y = count, fill = match)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = count, y = count/2)) +
  facet_grid(category~site)
```

The bar plots above are perhaps a bit difficult to interpret. What they are showing is the top five events detected in either the real 10, 20, or 30 year time series, but then those events are compared against the same event in the 30 year time series, and it is the category for that same event from the 30 year time series that is shown in the figure. For example, look at the event categories from the 10 year time series for the Northwest Atlantic (`NW_Atl`). We see that three of these events are 'I Moderate', and two are 'II Strong'. When we look at the events based on 30 years of data we see that all of the top five are 'II Strong'. This is because the 90th percentile threshold is higher when one uses only 10 years of data, so the events have a lower category. Oddly, this doesn't hold up in the Mediterranean time series.

With the differences in the count of categories for the detected events in the given time series lengths shown above, we will now compare the categories of the matching events between the different climatologies used.

```{r category_10_vs_30, fig.cap="Comparing the categories of the top 5 events detected with a 10 year climatology as oppossed to those detected with the standard 30 we see that the general pattern is that events are larger with the 30 year climatology period."}
sst_ALL_clim_category_10_top_5_compare <- rbind(data.frame(sst_ALL_clim_category_10_top_5, match = "clim_10"),
                                           data.frame(sst_ALL_clim_category_10_top_5_match, match = "clim_30")) %>% 
  group_by(site, match, category) %>% 
  summarise(count = n()) %>% 
  mutate(category = factor(category,
                           levels = c("I Moderate", "II Strong",
                                      "III Severe", "IV Extreme")))

ggplot(sst_ALL_clim_category_10_top_5_compare, aes(x = match, y = count, fill = match)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = count, y = count/2)) +
  facet_grid(category~site)
```

Now we are somewhat doing the opposite of the previous figure. In the figure above we are showing what the categories of the top five events in the 10 year time series really are, compared against what the matching events in the 30 year time series are. We see that the 30 year time series event categories are consistently larger to some degree.

```{r category_20_vs_30, fig.cap="Comparing the categories of the top 5 events detected with a 20 year climatology as oppossed to those detected with the standard 30 we see that the difference in the size of the categories detected with a 30 year climatology is less than when compared against a 10 year climatology."}
sst_ALL_clim_category_20_top_5_compare <- rbind(data.frame(sst_ALL_clim_category_20_top_5, match = "clim_20"),
                                           data.frame(sst_ALL_clim_category_20_top_5_match, match = "clim_30")) %>% 
  group_by(site, match, category) %>% 
  summarise(count = n()) %>% 
  mutate(category = factor(category,
                           levels = c("I Moderate", "II Strong",
                                      "III Severe", "IV Extreme")))

ggplot(sst_ALL_clim_category_20_top_5_compare, aes(x = match, y = count, fill = match)) +
  geom_bar(stat = "identity", position = position_dodge()) +
  geom_text(aes(label = count, y = count/2)) +
  facet_grid(category~site)
```


The above figure shows a similar pattern to the previous one, except that the discrepancy in category classifications between the top five events from 20 vs 30 year time periods is less than 10 vs 30 years. This is to be expected.

(RWS: As proposed by AJS, were this section of the analysis to be completed, the median and minimum five events should also be compared.)


## De-trended MHW detection

(RWS: I've not yet started on this section as I'm not convinced it should be included in this paper.)

(ECJO)[We clearly need to have a general conversation about this since I feel the opposite.]
(RWS)[I see more now why this is necessary. Perhaps rather than a de-trended results section, the whole paper should focus on de-trended data.]

## Arguments

With the amount of variance etc. that may be accounted for through re-sampling known, we will now look into how one may go about more confidently creating a climatology that will consistently detect events as similarly as possible by experimenting with how the various arguments within the detection pipeline may affect our results, given the different lengths of time series employed. After this has been done we will look into using the Fourier transform climatology generating method (https://robwschlegel.github.io/MHWdetection/articles/Climatologies_and_baselines.html) to see if that can't be more effective. The efficacy of these techniques will be judged through a number of statistical measurements of variance and similarity.

We will see how these different techniques may differ for corrections to both the 10 and 20 year time periods.

(RWS: I've not yet gotten into the manipulation of function arguments to tweak climatology/MHW metric precision.)


## Fourier transform climatologies

Lastly we will now go about reproducing all of the checks made above, but based on a climatology derived from a Fourier transformation, and not the default climatology creation method.

(RWS: I've not gotten to the Fourier transformations yet either. I'm also not convinced that this is going to be the way forward. I am thinking that it may be better to just increase the smoothing window width to produce a more sinusoidal climatology line. But I suppose that in order to know that it must be tested.)

(ECJO)[One advantage of the Fourier transform method (or, I think equivalently harmonic regression) is that it is VERY fast.  Much faster than the method used in the MHW code.  I also think you can probably get away with shorter time series using this method than with the “standard MHW climatology” method]

(RWS)[I suppose then that I will need to look into this.]

## Best practices

(RWS: I envision this last section distilling everything learned above into a nice bullet list. These bulletted items for each different vignette would then all be added up in one central [best practices](https://robwschlegel.github.io/MHWdetection/articles/best_practices.html) vignette that would be ported over into the final write-up in the discussion/best-practices section of the paper. Ideally these best-practices could also be incorporated into the R/python distributions of the MHW detection code to allow users to make use of the findings from the paper in their own work.)


### Options for improving climatology precision in short time series


### Options for improving MHW metric precision in short time series


## References

